import os
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.graphs import Neo4jGraph
from langchain.chains import GraphCypherQAChain
from dotenv import load_dotenv
from langchain_core.prompts import ChatPromptTemplate

# Load environment variables from .env file
load_dotenv()

# Print NEO4J_URI to verify if the .env is loaded correctly
neo4j_uri = os.getenv('NEO4J_URI')
print(neo4j_uri)

# Initialize the language model (Google Generative AI - Gemini 1.5)
llm = ChatGoogleGenerativeAI(model="gemini-1.5-pro", temperature=0.1)

# Initialize Neo4jGraph with correct URI, username, and password
graph = Neo4jGraph(
    url=neo4j_uri,
    username=os.getenv('NEO4J_USERNAME'),
    password=os.getenv('NEO4J_PASSWORD'),
    enhanced_schema=True,
)

# Neo4j query to load FAQ data from CSV
query = """ 
LOAD CSV WITH HEADERS FROM 'https://raw.githubusercontent.com/huynhanh48/Duan/master/FAQ.csv' AS row 
CREATE (user:userFAQ {question:row.Question, answer:row.Answer})
"""
# Execute the query to load FAQ data into the graph
graph.query(query)
graph.refresh_schema()

# Tạo prompt để tạo câu hỏi liên quan
translation_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "Thay vì chỉ mục 1, 2, 3, hãy sử dụng query với các dict.\
            Query gốc (origin) sẽ là query 0.\
            Nhiệm vụ của bạn là tạo ra 3 câu hỏi dựa trên truy vấn của người dùng.\
            Câu hỏi cần phải liên quan và có ý nghĩa đối với chủ đề truy vấn.\
            Không cần các trả lời không cần thiết.",
        ),
        ("human", "{input}"),
    ]
)

# Định dạng prompt đã được tạo thành danh sách các tin nhắn
formatted_prompt = translation_prompt.format_prompt(input='Dịch vụ Ngân hàng số VCB Digibank là gì?')

# Sau đó chuyển đổi prompt thành danh sách tin nhắn mà mô hình có thể xử lý
messages = formatted_prompt.to_messages()
print(messages)

# Truyền các tin nhắn vào mô hình ngôn ngữ và lấy phản hồi
response = llm({"messages": messages})
print(response.content)


# # Enable dangerous requests and create the GraphCypherQAChain
# chain = GraphCypherQAChain.from_llm(
#     graph=graph, 
#     llm=llm, 
#     verbose=True, 
#     allow_dangerous_requests=True  # Acknowledge dangerous requests
# )

# # Run a query to the chain using the content generated by the LLM
# response = chain.invoke({"query": perfomencechain})

# # Print the final response
# print(response)
